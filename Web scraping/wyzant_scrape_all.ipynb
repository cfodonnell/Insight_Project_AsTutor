{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import pickle\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:76.0) Gecko/20100101 Firefox/76.0'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_add = 'https://www.wyzant.com/match/search?sort=1&d=20&utc_offset=-5&min_price=10&max_price=200&min_age=18&max_age=100&gender_pref=none&st=5&kw=physics&ol=false&z='\n",
    "zip_code = 78750\n",
    "url = search_add + str(zip_code)\n",
    "\n",
    "chromedriver = '/home/codonnell/chromedriver.exe'\n",
    "driver = webdriver.Chrome(chromedriver)\n",
    "driver.get(url)\n",
    "\n",
    "#run this, then physically enter fields to get to search page\n",
    "#then reopen search url above and it should work this time\n",
    "#search for all tutor links on page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tutor_info(zip_code, links, headers):\n",
    "    \n",
    "    mains = []\n",
    "    scheds = []\n",
    "\n",
    "    empty = {'sunday': [None], 'monday': [None], 'tuesday': [None], 'wednesday': [None],\n",
    "         'thursday': [None], 'friday': [None], 'saturday': [None]}\n",
    "    \n",
    "    for i, link in enumerate(links):\n",
    "        r = requests.get(link, headers = headers).text\n",
    "        soup = BeautifulSoup(r, 'html.parser')\n",
    "\n",
    "        tags = soup.find_all('script')\n",
    "        try:\n",
    "            time_tutoring = soup.find_all('h3')[0].text\n",
    "        except:\n",
    "            time_tutoring = []\n",
    "        bio_list = soup.find_all('p', {'class':  'spc-zero-n spc-sm-s'})\n",
    "        bio = [part.text for part in bio_list]\n",
    "        \n",
    "        if soup.find_all('i' ,{'class':'wc-background-pass wc-green'}):\n",
    "            bck = 1\n",
    "            bck_passed = soup.find_all('p', {'class':\"spc-zero\"})[1].text\n",
    "        else:\n",
    "            bck = 0\n",
    "            bck_passed = []\n",
    "            \n",
    "        edu = soup.find_all('section' ,{'class':\"spc-med-s\"})\n",
    "        if edu:\n",
    "            educ =[e.text for e in edu]\n",
    "        else:\n",
    "            educ = []\n",
    "        \n",
    "        try:\n",
    "            main_details = json.loads(tags[0].string)\n",
    "            main_details['time_tutoring'] = time_tutoring\n",
    "            main_details['bio'] = bio\n",
    "            main_details['background_check'] = bck\n",
    "            main_details['date_background_passed'] = bck_passed\n",
    "            main_details['education'] = educ\n",
    "        \n",
    "            mains.append(main_details)\n",
    "            try:\n",
    "                scheds.append(json.loads(tags[6].string.split('=')[1][:-1]))\n",
    "            except:\n",
    "                scheds.append(empty)\n",
    "        except:\n",
    "            pass\n",
    "                \n",
    "        time.sleep(np.random.rand()*5)\n",
    "        \n",
    "    df = pd.merge(pd.DataFrame(mains), pd.DataFrame(scheds), left_index=True, right_index=True)\n",
    "    df.to_csv('/home/codonnell/tutors/tutnew/tutors_' + str(zip_code) + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_tut_id(unique_tuts):\n",
    "    with open('/home/codonnell/tutors_ids.txt', 'w') as filehandle:\n",
    "        for item in unique_tuts:\n",
    "            filehandle.write('%s\\n' % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([30004, 30016, 30022, 30024, 30032, 30034, 30039, 30040, 30041,\n",
       "       30043, 30047, 30052, 30058, 30062, 30064, 30066, 30067, 30075,\n",
       "       30076, 30080, 30083, 30093, 30096, 30097, 30101, 30114, 30127,\n",
       "       30134, 30135, 30144, 30157, 30188, 30236, 30253, 30263, 30281,\n",
       "       30318, 30331, 30349, 30506, 30518, 30606, 30721, 30906, 30907,\n",
       "       30909, 31088, 31419, 31907, 80003, 80004, 80010, 80011, 80012,\n",
       "       80013, 80014, 80015, 80016, 80017, 80020, 80021, 80022, 80027,\n",
       "       80031, 80111, 80122, 80123, 80126, 80127, 80128, 80134, 80138,\n",
       "       80204, 80205, 80210, 80211, 80219, 80220, 80221, 80226, 80227,\n",
       "       80228, 80229, 80231, 80233, 80239, 80241, 80260, 80401, 80501,\n",
       "       80503, 80504, 80524, 80525, 80526, 80537, 80538, 80601, 80631,\n",
       "       80634, 80906, 80909, 80911, 80916, 80917, 80918, 80920, 81001,\n",
       "       81005, 81007, 81504, 98003, 98004, 98006, 98012, 98023, 98026,\n",
       "       98030, 98031, 98032, 98033, 98034, 98036, 98042, 98052, 98056,\n",
       "       98058, 98059, 98092, 98103, 98105, 98115, 98118, 98125, 98133,\n",
       "       98155, 98168, 98198, 98203, 98204, 98208, 98223, 98225, 98226,\n",
       "       98258, 98270, 98277, 98366, 98374, 98387, 98391, 98444, 98501,\n",
       "       98503, 98513, 98584, 98604, 98632, 98661, 98682, 98801, 98837,\n",
       "       98902, 98908, 99205, 99206, 99208, 99301, 99336, 99362])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "already_done = [77449,11368,60629,90011,79936,30044,80209,89138,28208,84102,98122,33128,\n",
    "               46201, 55401,43215,19102,94108,22046,91911,37128,85001,78214,48201,63108,\n",
    "               80303, 926, 8701]\n",
    "pop_zips = pd.read_csv('/home/codonnell/Insight/Insight_Project_AsTutor/data/raw/zips_incomes/zips_ga_co_wa.csv')\n",
    "\n",
    "np.setdiff1d(pop_zips['zip_code'].values,already_done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "zips = np.setdiff1d(pop_zips['zip_code'].values,already_done)[-7:]\n",
    "#zips = already_done\n",
    "subjects = ['physics', 'math', 'spanish', 'english', 'computer']\n",
    "links = []\n",
    "tot_tuts = []\n",
    "\n",
    "for zip_code in zips:\n",
    "    links = []\n",
    "    tot_tuts = []\n",
    "    \n",
    "    for subject in subjects:\n",
    "            \n",
    "        url = 'https://www.wyzant.com/match/search?sort=1&d=20&utc_offset=-5&min_price=10&max_price=200&min_age=18&max_age=100&gender_pref=none&st=0&kw=' + subject + '&ol=false&z=' + str(zip_code)\n",
    "        driver.get(url)\n",
    "        \n",
    "        try:\n",
    "            num_tuts = driver.find_elements_by_tag_name('strong')[2].text #per subject given zip\n",
    "        except:\n",
    "            num_tuts = 0\n",
    "        \n",
    "        lks = driver.find_elements_by_xpath('.//a[@class=\"tutor-card flex\"]')\n",
    "        lks_found = [link.get_attribute('href') for link in lks]\n",
    "        links.append(lks_found)\n",
    "        \n",
    "        try:\n",
    "            if int(num_tuts.split(' ')[0]) > 10:\n",
    "                extras = int(np.floor(int(num_tuts.split(' ')[0])/10)) + 1\n",
    "                for i in range(1,extras):\n",
    "                    urlmore = 'https://www.wyzant.com/match/search/more?d=20&gender_pref=none&kw=' + subject + '&max_age=100&max_price=500&min_age=18&min_price=10&ol=false&page_number=' + str(i) + '&sort=1&st=0&utc_offset=-5&z=' +str(zip_code)\n",
    "                    driver.get(urlmore)\n",
    "                    more_lks = driver.find_elements_by_xpath('.//a[@class=\"tutor-card flex\"]')\n",
    "                    more_lks_found = [link.get_attribute('href') for link in more_lks]\n",
    "                    links.append(more_lks_found)\n",
    "                    time.sleep(np.random.rand()*5)\n",
    "            else:\n",
    "                pass\n",
    "        except:\n",
    "            pass\n",
    "            \n",
    "            \n",
    "        tot_tuts.append(num_tuts) #per subject given zip\n",
    "            \n",
    "            \n",
    "        time.sleep(np.random.rand()*5)\n",
    "    \n",
    "    unique_tuts = np.unique(np.array([item for sublist in links for item in sublist]))\n",
    "    save_tut_id(unique_tuts)\n",
    "    get_tutor_info(zip_code, unique_tuts, headers)\n",
    "    tut_num_df = pd.DataFrame({'zip_code': [str(zip_code)]*5, 'tot_tuts': tot_tuts})\n",
    "    tut_num_df.to_csv('/home/codonnell/tutors//tutnew/tut_num_' +str(zip_code) + '.csv')\n",
    "    \n",
    "    time.sleep(np.random.rand()*5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3031,  3032,  3038,  3042,  3045,  3049,  3051,  3052,  3053,\n",
       "        3054,  3055,  3060,  3062,  3063,  3064,  3076,  3077,  3079,\n",
       "        3087,  3102,  3103,  3104,  3106,  3109,  3110,  3220,  3222,\n",
       "        3229,  3235,  3244,  3246,  3249,  3253,  3264,  3275,  3276,\n",
       "        3281,  3301,  3303,  3304,  3307,  3431,  3446,  3452,  3458,\n",
       "        3470,  3561,  3570,  3743,  3755,  3766,  3773,  3801,  3811,\n",
       "        3820,  3824,  3825,  3833,  3835,  3841,  3842,  3848,  3857,\n",
       "        3865,  3867,  3868,  3873,  3874,  3878,  3885,  5001,  5032,\n",
       "        5033,  5048,  5055,  5060,  5068,  5089,  5091,  5101,  5143,\n",
       "        5149,  5156,  5201,  5250,  5255,  5257,  5261,  5262,  5301,\n",
       "        5346,  5354,  5401,  5403,  5404,  5408,  5440,  5443,  5445,\n",
       "        5446,  5450,  5452,  5454,  5458,  5461,  5462,  5464,  5465,\n",
       "        5468,  5476,  5477,  5478,  5482,  5488,  5489,  5491,  5495,\n",
       "        5602,  5641,  5655,  5656,  5661,  5663,  5667,  5672,  5673,\n",
       "        5676,  5677,  5679,  5680,  5701,  5733,  5735,  5743,  5753,\n",
       "        5759,  5763,  5764,  5765,  5777,  5819,  5851,  5855,  5860,\n",
       "        6010,  6033,  6040,  6051,  6053,  6066,  6074,  6082,  6095,\n",
       "        6106,  6108,  6109,  6111,  6114,  6118,  6320,  6340,  6360,\n",
       "        6405,  6410,  6450,  6457,  6460,  6473,  6484,  6489,  6492,\n",
       "        6511,  6512,  6513,  6514,  6516,  6604,  6606,  6611,  6614,\n",
       "        6704,  6705,  6708,  6770,  6776,  6790,  6810,  6811,  6824,\n",
       "        6851,  6854,  6877,  6880,  6902, 55402, 55403, 55404, 55405,\n",
       "       55406, 55407, 55408, 55409, 55410, 55411, 55412, 55413, 55414,\n",
       "       55415, 55416, 55417, 55418, 55419, 55420, 55421, 55422, 55423,\n",
       "       55424, 55425, 55426, 55427, 55428, 55429, 55430, 55431, 55432,\n",
       "       55433, 55434, 55435, 55436, 55437, 55438, 55439, 55441, 55442,\n",
       "       55443, 55444, 55445, 55446, 55447, 55448, 55449, 55454])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "already_done = [77449,11368,60629,90011,79936,30044,80209,89138,28208,84102,98122,33128,\n",
    "               46201, 55401,43215,19102,94108,22046,91911,37128,85001,78214,48201,63108,\n",
    "               80303, 926, 8701]\n",
    "state = 'mi_ct_vt_nh'\n",
    "pop_zips = pd.read_csv('/home/codonnell/Insight/Insight_Project_AsTutor/data/raw/zips_incomes/zips_' + state + '.csv')\n",
    "\n",
    "np.setdiff1d(pop_zips['zip_code'].values,already_done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "zips = np.setdiff1d(pop_zips['zip_code'].values,already_done)\n",
    "#zips = already_done\n",
    "subjects = ['physics', 'math', 'spanish', 'english', 'computer']\n",
    "links = []\n",
    "tot_tuts_dfs = []\n",
    "\n",
    "for zip_code in zips:\n",
    "    tot_tuts = []\n",
    "    \n",
    "    for subject in subjects:\n",
    "            \n",
    "        url = 'https://www.wyzant.com/match/search?sort=1&d=40&utc_offset=-5&min_price=10&max_price=200&min_age=18&max_age=100&gender_pref=none&st=0&kw=' + subject + '&ol=false&z=' + str(zip_code)\n",
    "        driver.get(url)\n",
    "        \n",
    "        try:\n",
    "            num_tuts = driver.find_elements_by_tag_name('strong')[2].text #per subject given zip\n",
    "        except:\n",
    "            num_tuts = 0\n",
    "        \n",
    "        lks = driver.find_elements_by_xpath('.//a[@class=\"tutor-card flex\"]')\n",
    "        lks_found = [link.get_attribute('href') for link in lks]\n",
    "        links.append(lks_found)\n",
    "        \n",
    "        try:\n",
    "            if int(num_tuts.split(' ')[0]) > 10:\n",
    "                extras = int(np.floor(int(num_tuts.split(' ')[0])/10)) + 1\n",
    "                for i in range(1,extras):\n",
    "                    urlmore = 'https://www.wyzant.com/match/search/more?d=40&gender_pref=none&kw=' + subject + '&max_age=100&max_price=500&min_age=18&min_price=10&ol=false&page_number=' + str(i) + '&sort=1&st=0&utc_offset=-5&z=' +str(zip_code)\n",
    "                    driver.get(urlmore)\n",
    "                    more_lks = driver.find_elements_by_xpath('.//a[@class=\"tutor-card flex\"]')\n",
    "                    more_lks_found = [link.get_attribute('href') for link in more_lks]\n",
    "                    links.append(more_lks_found)\n",
    "                    time.sleep(np.random.rand()*5)\n",
    "            else:\n",
    "                pass\n",
    "        except:\n",
    "            pass\n",
    "            \n",
    "            \n",
    "        tot_tuts.append(num_tuts) #per subject given zip\n",
    "        time.sleep(np.random.rand()*5)\n",
    "        \n",
    "    tot_tuts_dfs.append(pd.DataFrame({'zip_code': [str(zip_code)]*5, 'tot_tuts': tot_tuts}))\n",
    "    time.sleep(np.random.rand()*5)      \n",
    "            \n",
    "    \n",
    "unique_tuts = np.unique(np.array([item for sublist in links for item in sublist]))\n",
    "save_tut_id(unique_tuts)\n",
    "get_tutor_info(state, unique_tuts, headers)\n",
    "tut_num_df = pd.concat(tot_tuts_dfs)\n",
    "tut_num_df.to_csv('/home/codonnell/tutors//tutnew/tut_num_' + state + '.csv')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#driver.get(url)\n",
    "num_tuts = driver.find_elements_by_tag_name('strong')[2].text #per subject given zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_tutor_info(state, unique_tuts, headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
